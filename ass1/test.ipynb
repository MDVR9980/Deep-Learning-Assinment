{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_cell"
   },
   "source": [
    "# Telegram Sentiment Analysis using ParsBERT\n",
    "\n",
    "This notebook is designed to perform sentiment analysis on Persian texts extracted from Telegram channels. It includes steps for data loading, preprocessing, sentiment prediction using a pre-trained model, temporal analysis, and saving the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_header"
   },
   "source": [
    "### 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# üß© 1. Install Required Libraries (Patched & Unified Version)\n",
    "# ==============================================================\n",
    "# This block fixes compatibility issues with 'hazm' and ensures a stable installation.\n",
    "\n",
    "print(\"‚è≥ Step 1: Upgrading Python build tools...\")\n",
    "# First, upgrade pip, setuptools, and wheel to prevent build errors.\n",
    "!pip install --upgrade -q pip setuptools wheel\n",
    "\n",
    "print(\"‚è≥ Step 2: Cloning and patching 'hazm' for compatibility...\")\n",
    "# The standard 'hazm' library has compatibility issues with recent Python versions in Colab.\n",
    "# We clone it and patch its configuration file to remove restrictive version constraints.\n",
    "!git clone https://github.com/sobhe/hazm.git &> /dev/null\n",
    "!sed -i 's/python = \">=3.8, <3.12\"/python = \">=3.8\"/' /content/hazm/pyproject.toml\n",
    "!sed -i 's/numpy = \"==1.24.3\"/numpy = \">=1.24.3\"/' /content/hazm/pyproject.toml\n",
    "\n",
    "print(\"‚è≥ Step 3: Installing all required libraries...\")\n",
    "# Install the patched local version of hazm along with all other libraries in a single command.\n",
    "# This allows pip to resolve all dependencies correctly.\n",
    "!pip install -q /content/hazm/ \\\n",
    "    transformers==4.36.2 \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    accelerate \\\n",
    "    scikit-learn \\\n",
    "    pandas \\\n",
    "    matplotlib \\\n",
    "    seaborn \\\n",
    "    emoji \\\n",
    "    torch \\\n",
    "    huggingface_hub\n",
    "\n",
    "# =========================\n",
    "# 4. Verify Installation\n",
    "# =========================\n",
    "# This step tries to import the libraries to confirm the installation was successful.\n",
    "try:\n",
    "    import hazm\n",
    "    import emoji\n",
    "    import transformers\n",
    "    print(\"\\n‚úÖ All libraries installed and imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Installation failed. An essential library could not be imported: {e}\")\n",
    "    print(\"Please restart the runtime (Runtime > Restart runtime) and try running this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_header"
   },
   "source": [
    "### 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_cell"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. IMPORT LIBRARIES\n",
    "# =========================\n",
    "import os, re, json, glob, emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from hazm import Normalizer\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drive_header"
   },
   "source": [
    "### 3. Mount Google Drive and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_cell"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. MOUNT DRIVE\n",
    "# =========================\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# IMPORTANT: Change this path to your data folder in Google Drive\n",
    "DATA_FOLDER = \"/content/drive/MyDrive/telegram-sentiment-analysis-fa/data\"\n",
    "RESULTS_FOLDER = \"/content/drive/MyDrive/telegram-sentiment-analysis-fa/results\"\n",
    "\n",
    "# Create the results folder if it doesn't exist\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "print(f\"Data will be read from: {DATA_FOLDER}\")\n",
    "print(f\"Results will be saved to: {RESULTS_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_header"
   },
   "source": [
    "### 4. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_cell"
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# 4. LOAD AND MERGE TELEGRAM CSV FILES\n",
    "# =======================================\n",
    "print(f\"üìÇ Reading CSV files from: {DATA_FOLDER}\")\n",
    "csv_files = glob.glob(os.path.join(DATA_FOLDER, \"*.csv\"))\n",
    "dfs = []\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"‚ö†Ô∏è No CSV files found in the specified path. Please check the DATA_FOLDER variable.\")\n",
    "else:\n",
    "    for f in csv_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(f)\n",
    "            # Extract channel name from the filename\n",
    "            channel_name = os.path.basename(f).replace(\"_messages.csv\", \"\")\n",
    "            temp_df['channel'] = channel_name\n",
    "            temp_df['source_file'] = os.path.basename(f)\n",
    "            dfs.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not dfs:\n",
    "        raise SystemExit(\"‚ùå No dataframes were loaded. Halting execution.\")\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"üìä Final dataframe created with {len(df)} rows.\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocess_header"
   },
   "source": [
    "### 5. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess_cell"
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5. PERSIAN TEXT PREPROCESSING\n",
    "# =============================\n",
    "normalizer = Normalizer()\n",
    "\n",
    "def preprocess_persian(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove HTML tags and links\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    # Normalize with Hazm\n",
    "    text = normalizer.normalize(text)\n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    # Remove unnecessary characters (keeps Persian alphabet, spaces, and ZWNJ)\n",
    "    text = re.sub(r'[^\\w\\sÿ¢ÿßÿ®Ÿæÿ™ÿ´ÿ¨⁄Üÿ≠ÿÆÿØÿ∞ÿ±ÿ≤⁄òÿ≥ÿ¥ÿµÿ∂ÿ∑ÿ∏ÿπÿ∫ŸÅŸÇ⁄©⁄ØŸÑŸÖŸÜŸàŸá€åŸî‚Äå-]', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"üßπ Preprocessing the 'post_text' column...\")\n",
    "df['clean_post_text'] = df['post_text'].apply(preprocess_persian)\n",
    "\n",
    "# Drop rows where the cleaned text is empty\n",
    "df.dropna(subset=['clean_post_text'], inplace=True)\n",
    "df = df[df['clean_post_text'] != '']\n",
    "\n",
    "print(\"Sample of original vs. cleaned text:\")\n",
    "display(df[['post_text', 'clean_post_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis_header"
   },
   "source": [
    "### 6. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis_cell"
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6. DEFINE MODEL AND LABELS FOR SENTIMENT ANALYSIS\n",
    "# =====================================================\n",
    "# We will use a pre-trained model for sentiment analysis.\n",
    "# This model classifies text into \"positive\", \"negative\", and \"neutral\" categories.\n",
    "SENT_MODEL_NAME = \"HooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary\"\n",
    "device = 0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=SENT_MODEL_NAME, device=device)\n",
    "\n",
    "# Define final labels for classification\n",
    "label_list = [\"happy\", \"sad\", \"neutral\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    results = []\n",
    "    # To prevent memory errors, we process the data in batches.\n",
    "    # Using the pipeline directly on a list is highly efficient.\n",
    "    predictions = sentiment_pipeline(texts, batch_size=64, truncation=True)\n",
    "    for out in predictions:\n",
    "        label = out['label'].lower()\n",
    "        if 'pos' in label:\n",
    "            results.append('happy')\n",
    "        elif 'neg' in label:\n",
    "            results.append('sad')\n",
    "        else:\n",
    "            results.append('neutral')\n",
    "    return results\n",
    "\n",
    "print(f\"üß† Sentiment model loaded on device: {'GPU' if device == 0 else 'CPU'}. Predicting sentiment for all messages...\")\n",
    "# Use the cleaned text for prediction\n",
    "texts_to_analyze = df['clean_post_text'].tolist()\n",
    "df['pred_label'] = predict_sentiment(texts_to_analyze)\n",
    "df['pred_label_id'] = df['pred_label'].map(label2id)\n",
    "\n",
    "print(\"\\n‚úÖ Sentiment analysis completed successfully.\")\n",
    "print(\"\\nDistribution of predicted sentiments:\")\n",
    "print(df['pred_label'].value_counts())\n",
    "\n",
    "print(\"\\nSample of predictions:\")\n",
    "display(df[['clean_post_text', 'pred_label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "temporal_header"
   },
   "source": [
    "### 7. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "temporal_cell"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. TEMPORAL ANALYSIS\n",
    "# =========================\n",
    "print(\"üìà Preparing for temporal analysis...\")\n",
    "# Convert timestamp column to datetime format, coercing errors to NaT (Not a Time)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df.dropna(subset=['timestamp'], inplace=True)\n",
    "\n",
    "if not df.empty:\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    df['month'] = df['timestamp'].dt.to_period('M')\n",
    "\n",
    "    # Monthly analysis\n",
    "    monthly_trends = df.groupby(['month', 'pred_label']).size().unstack(fill_value=0)\n",
    "    monthly_trends = monthly_trends.sort_index()\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    for label in label_list:\n",
    "        if label in monthly_trends.columns:\n",
    "            plt.plot(monthly_trends.index.to_timestamp(), monthly_trends[label], label=label, marker='o', linestyle='-')\n",
    "    plt.legend()\n",
    "    plt.title(\"Monthly Sentiment Trends in Telegram Posts\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Number of Posts\")\n",
    "    plt.show()\n",
    "\n",
    "    # Yearly analysis\n",
    "    yearly_dist = df.groupby(['year', 'pred_label']).size().unstack(fill_value=0)\n",
    "    yearly_dist.plot(kind='bar', stacked=True, figsize=(12, 7))\n",
    "    plt.title(\"Yearly Sentiment Distribution\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Number of Posts\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid timestamps found for temporal analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_header"
   },
   "source": [
    "### 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_cell"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. SAVE RESULTS\n",
    "# =========================\n",
    "output_csv_path = os.path.join(RESULTS_FOLDER, \"telegram_posts_with_sentiment.csv\")\n",
    "output_json_path = os.path.join(RESULTS_FOLDER, \"telegram_posts_with_sentiment.json\")\n",
    "\n",
    "print(f\"üíæ Saving results to: {RESULTS_FOLDER}\")\n",
    "# Save CSV with proper encoding for Persian characters\n",
    "df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Save JSON with proper encoding for Persian characters\n",
    "df_for_json = df[['clean_post_text', 'pred_label', 'timestamp', 'channel']]\n",
    "df_for_json.to_json(output_json_path, orient='records', force_ascii=False, lines=True, date_format='iso')\n",
    "\n",
    "print(f\"\\nüéâ Process finished! Results have been saved to the following files:\")\n",
    "print(f\"- {output_csv_path}\")\n",
    "print(f\"- {output_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
