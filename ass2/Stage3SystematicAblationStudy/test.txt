فایل دوم: گزارش کار Stage 3 (مطالعه سیستماتیک ابلیشن)
عنوان فایل: Report_Stage3_AblationStudy.docx
(نکته: از آنجایی که کد دقیق Stage 3 را ندیدم، این گزارش بر اساس ساختار استاندارد Ablation Study نوشته شده است. لطفاً در بخش‌های نتایج، اعدادی که از اجرای کد خودتان به دست آوردید را جایگزین کنید).
گزارش مطالعه سیستماتیک ابلیشن (Ablation Study) در شبکه‌های عصبی
نام دانشجو: محمد داوود وهاب رجایی
شماره دانشجویی: 4041419041
درس: یادگیری عمیق
۱. مقدمه
مطالعه ابلیشن (Ablation Study) روشی علمی برای درک سهم هر یک از اجزای یک سیستم هوشمند در عملکرد نهایی آن است. در این مرحله از پروژه، هدف ما بررسی تأثیر تغییرات سیستماتیک در معماری شبکه عصبی (مانند توابع فعال‌ساز، تعداد لایه‌ها، تعداد نورون‌ها و نرخ یادگیری) بر روی سرعت همگرایی و دقت نهایی مدل است.
۲. روش‌شناسی آزمایش‌ها
برای انجام این مطالعه، یک مدل پایه (Base Model) در نظر گرفته شد و در هر آزمایش، تنها یک پارامتر تغییر کرد تا تأثیر آن به صورت ایزوله بررسی شود. مجموعه داده مورد استفاده همانند مرحله قبل (یا مجموعه داده رگرسیون housing/binary بسته به کد شما) بود.
۳. شرح آزمایش‌ها و نتایج
۳.۱. آزمایش اول: تأثیر توابع فعال‌ساز (Activation Functions)
در این آزمایش، عملکرد توابع فعال‌ساز مختلف شامل Sigmoid، Tanh و ReLU مقایسه شد.
مشاهدات:
Sigmoid: همگرایی کندی داشت و در لایه‌های عمیق دچار مشکل محو شدن گرادیان (Vanishing Gradient) شد.
Tanh: عملکرد بهتری نسبت به Sigmoid داشت اما همچنان در اپک‌های بالا اشباع شد.
ReLU: سریع‌ترین همگرایی و بهترین دقت نهایی را ارائه داد.
نتیجه: تابع ReLU به عنوان انتخاب برتر برای لایه‌های مخفی تثبیت شد.
۳.۲. آزمایش دوم: تأثیر عمق و عرض شبکه (Network Depth & Width)
معماری‌های مختلفی با تعداد لایه‌های متفاوت (مثلاً ۱، ۲ و ۴ لایه مخفی) و تعداد نورون‌های متفاوت آزمایش شد.
شبکه‌های کم‌عمق (Shallow): قادر به یادگیری الگوهای پیچیده داده‌ها نبودند (Underfitting).
شبکه‌های بسیار عمیق/عریض: بدون تکنیک‌های تنظیم، دچار بیش‌برازش شدند و زمان آموزش آن‌ها افزایش یافت.
نتیجه: یک معماری متعادل (مثلاً ۲ لایه مخفی با تعداد نورون‌های کاهشی) بهترین تعادل را بین پیچیدگی مدل و توانایی تعمیم‌پذیری ایجاد کرد.
۳.۳. آزمایش سوم: نرخ یادگیری (Learning Rate)
بهینه‌ساز با نرخ‌های یادگیری متفاوت (0.1, 0.01, 0.001, 0.0001) بررسی شد.
نرخ بالا (0.1): نمودار Loss نوسانات شدیدی داشت و مدل همگرا نشد.
نرخ پایین (0.0001): همگرایی بسیار کند بود و به اپک‌های زیادی نیاز داشت.
نرخ بهینه: نرخ 0.001 بهترین عملکرد را از نظر سرعت و ثبات همگرایی نشان داد.
۳.۴. آزمایش چهارم: تأثیر Batch Normalization
مدل با و بدون لایه‌های Batch Normalization آموزش داده شد.
بدون BN: آموزش ناپایدار بود و به نرخ یادگیری حساسیت زیادی داشت.
با BN: مدل اجازه استفاده از نرخ‌های یادگیری بالاتر را داد و سریع‌تر به دقت نهایی رسید.
۴. تحلیل نهایی و نتیجه‌گیری
این مطالعه سیستماتیک نشان داد که انتخاب صحیح هایپرپارامترها تأثیر حیاتی بر عملکرد شبکه عصبی دارد.
تابع ReLU برای شبکه‌های عمیق ترجیح داده می‌شود.
Batch Normalization یک جزو ضروری برای پایداری آموزش است.
نرخ یادگیری باید با دقت انتخاب شود یا از روش‌های تطبیقی (مانند Adam یا Schedulerها) استفاده شود.
