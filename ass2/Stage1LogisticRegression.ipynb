{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 24px; font-weight: bold;\">In the name of God, the Most Gracious, the Most Merciful</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPBG28rs5i1C"
      },
      "source": [
        "Full Name: MohammadDavood VahhabRajaee\n",
        "\n",
        "Student ID: 4041419041"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_9TC5gnxxzf"
      },
      "source": [
        "# Logistic Regression from Scratch using NumPy\n",
        "**what is logistic regression?**\n",
        "\n",
        "Logistic regression is a supervised learning method used for binary classification â€” predicting outcomes that can take only two values (e.g., 0/1, yes/no, positive/negative).\n",
        "\n",
        "It models the probability that an input belongs to a particular class.\n",
        "\n",
        "Logistic regression is helpful when you want to predict which of two categories an input belongs to, and when the relationship between the features and the log-probability of the outcome is approximately linear.\n",
        "\n",
        "**Note: In the code section, complete the `# TODO: implement this` placeholder with the required functionality. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rIpJnNzmudK6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iyperjty25i"
      },
      "source": [
        "## 1.Dataset Overview\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-tLze6ey_3r"
      },
      "source": [
        "### practice 1\n",
        "- Load the provided dataset (`binary.csv`)\n",
        "- How many samples it has? Are labels balanced? what are the labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmrX4ZdwxGry",
        "outputId": "10aa6969-5704-43f1-a34a-f0c6eb3cfe88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Data loaded successfully!\n",
            "ðŸ“Š Shape: (400, 4)\n",
            "\n",
            "ðŸŽ¯ Target (admit) distribution:\n",
            "admit\n",
            "0    273\n",
            "1    127\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Admission rate: 31.8%\n",
            "\n",
            "ðŸŽ“ Rank distribution:\n",
            "rank\n",
            "1     61\n",
            "2    151\n",
            "3    121\n",
            "4     67\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the dataset from the CSV file\n",
        "data = pd.read_csv('binary.csv')\n",
        "print(\"âœ… Data loaded successfully!\")\n",
        "\n",
        "# 2. Display the first 5 rows to inspect structure and data types\n",
        "# print(\"\\nFirst 5 rows:\")\n",
        "# print(data.head())\n",
        "\n",
        "# 3. Print the shape (number of rows, number of columns)\n",
        "print(f\"ðŸ“Š Shape: {data.shape}\\n\")\n",
        "\n",
        "# 4. Analyze the target variable 'admit' to check for balance\n",
        "print(\"ðŸŽ¯ Target (admit) distribution:\")\n",
        "admit_counts = data['admit'].value_counts()\n",
        "print(admit_counts)\n",
        "admission_rate = data['admit'].mean()\n",
        "print(f\"\\nAdmission rate: {admission_rate:.1%}\\n\")\n",
        "\n",
        "# 5. Examine the 'rank' feature distribution to understand it better\n",
        "print(\"ðŸŽ“ Rank distribution:\")\n",
        "rank_counts = data['rank'].value_counts().sort_index()\n",
        "print(rank_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08MmCtWd2RMw"
      },
      "source": [
        "### practice 2: Preprocess â€” Add Bias- Normalize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtqp_0fFzq2C",
        "outputId": "6ce7a60a-661e-44eb-889b-bad05c1793d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Raw data loaded:\n",
            "   X_raw shape: (400, 2) | y shape: (400,)\n",
            "   gre range: [220.0, 800.0]\n",
            "   gpa range: [2.26, 4.00]\n",
            "\n",
            "âœ… Train-test split:\n",
            "   Train: 320 samples (admit rate: 31.87%)\n",
            "   Test:  80 samples (admit rate: 31.25%)\n",
            "\n",
            "âœ… Standardization (using TRAIN statistics):\n",
            "   gre: Î¼ = 588.75, Ïƒ = 118.63\n",
            "   gpa: Î¼ = 3.40, Ïƒ = 0.38\n",
            "\n",
            "âœ… Final feature matrices:\n",
            "   X_train shape: (320, 3) | feature order: [bias, gre_std, gpa_std]\n",
            "   X_test  shape: (80, 3)\n",
            "   First train sample: [ 1.         -0.0737578   0.00493654]\n"
          ]
        }
      ],
      "source": [
        "# 1. Extract features (gre, gpa) and target (admit)\n",
        "# We exclude 'rank' for this part of the assignment.\n",
        "X_raw = data[['gre', 'gpa']].values\n",
        "y = data['admit'].values\n",
        "\n",
        "print(\"âœ… Raw data loaded:\")\n",
        "print(f\"   X_raw shape: {X_raw.shape} | y shape: {y.shape}\")\n",
        "print(f\"   gre range: [{X_raw[:, 0].min()}, {X_raw[:, 0].max()}]\")\n",
        "print(f\"   gpa range: [{X_raw[:, 1].min():.2f}, {X_raw[:, 1].max():.2f}]\")\n",
        "\n",
        "\n",
        "# 2. Train-Test Split (80% train, 20% test)\n",
        "# We use stratify=y to ensure the proportion of admitted/rejected is the same in both sets.\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X_raw, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Train-test split:\")\n",
        "print(f\"   Train: {len(y_train)} samples (admit rate: {y_train.mean():.2%})\")\n",
        "print(f\"   Test:  {len(y_test)} samples (admit rate: {y_test.mean():.2%})\")\n",
        "\n",
        "\n",
        "# 3. Standardize features (Z-score normalization)\n",
        "# IMPORTANT: Calculate mean and std ONLY from the training data.\n",
        "means = np.mean(X_train_raw, axis=0)\n",
        "stds = np.std(X_train_raw, axis=0)\n",
        "\n",
        "# Standardize the training set\n",
        "X_train_std = (X_train_raw - means) / stds\n",
        "\n",
        "# Standardize the test set using the *training* statistics to prevent data leakage\n",
        "X_test_std = (X_test_raw - means) / stds\n",
        "\n",
        "print(\"\\nâœ… Standardization (using TRAIN statistics):\")\n",
        "print(f\"   gre: Î¼ = {means[0]:.2f}, Ïƒ = {stds[0]:.2f}\")\n",
        "print(f\"   gpa: Î¼ = {means[1]:.2f}, Ïƒ = {stds[1]:.2f}\")\n",
        "\n",
        "\n",
        "# 4. Add bias term (intercept)\n",
        "# This adds a column of ones to the beginning of the feature matrices.\n",
        "X_train = np.c_[np.ones(X_train_std.shape[0]), X_train_std]\n",
        "X_test = np.c_[np.ones(X_test_std.shape[0]), X_test_std]\n",
        "\n",
        "print(\"\\nâœ… Final feature matrices:\")\n",
        "print(f\"   X_train shape: {X_train.shape} | feature order: [bias, gre_std, gpa_std]\")\n",
        "print(f\"   X_test  shape: {X_test.shape}\")\n",
        "print(f\"   First train sample: {X_train[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1_IBWes3EeQ"
      },
      "source": [
        "## 2.From Linear to Logistic Regression\n",
        "\n",
        "Recap: linear regression ( you have implemented it in the last notebook)\n",
        "\n",
        "$\\hat{y} = Xw$\n",
        "\n",
        "Problem â€” output is unbounded.\n",
        "\n",
        "Logistic regression:\n",
        "\n",
        "$\\hat{y} = \\sigma(Xw)$\n",
        "\n",
        "Where sigmoid is:\n",
        "\n",
        "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKBNKLOo30Xe"
      },
      "source": [
        "### practice 3: implement sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OrfnLjb02jp4"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Applies the sigmoid (logistic) function element-wise to input `z`.\n",
        "    \"\"\"\n",
        "    # The formula for the sigmoid function is 1 divided by (1 + e^(-z))\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEKXzD5NlPd5",
        "outputId": "cf15fee2-6820-43a0-e175-3483b49ca724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sigmoid(0) = 0.5\n",
            "sigmoid(-10) = 4.5397868702434395e-05\n",
            "sigmoid(10) = 0.9999546021312976\n"
          ]
        }
      ],
      "source": [
        "print(\"sigmoid(0) =\", sigmoid(0))\n",
        "print(\"sigmoid(-10) =\", sigmoid(-10))\n",
        "print(\"sigmoid(10) =\", sigmoid(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4EZ7Jw97EmL"
      },
      "source": [
        "## 3.Loss Function\n",
        "\n",
        "Binary cross entropy:\n",
        "\n",
        "$J(w) = -\\frac{1}{m} \\sum \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyKzEL8Z7kgj"
      },
      "source": [
        "### practice 4: Implement cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VycaJ1oA4HgY"
      },
      "outputs": [],
      "source": [
        "def compute_loss(X, y, w):\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy (log) loss for logistic regression.\n",
        "    \"\"\"\n",
        "    # Get the number of samples\n",
        "    m = len(y)\n",
        "    \n",
        "    # 1. Calculate the linear combination (z = X * w)\n",
        "    z = X @ w\n",
        "    \n",
        "    # 2. Apply the sigmoid function to get predicted probabilities (y_hat)\n",
        "    y_hat = sigmoid(z)\n",
        "\n",
        "    # 3. Compute the binary cross-entropy loss\n",
        "    # The formula is the mean of -[y*log(y_hat) + (1-y)*log(1-y_hat)]\n",
        "    # A small epsilon (1e-15) is added to prevent log(0) errors for numerical stability\n",
        "    loss = -np.mean(y * np.log(y_hat + 1e-15) + (1 - y) * np.log(1 - y_hat + 1e-15))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CpeFg1K8PLh"
      },
      "source": [
        "## 4.Gradient Descent\n",
        "\n",
        "Gradient:\n",
        "\n",
        "$\\frac{\\partial J}{\\partial w} = \\frac{1}{m} X^T (\\hat{y} - y)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW77y_Ky8UhK"
      },
      "source": [
        "### practice 5: Implement gradient descent and Train with lr=0.0001, 60k steps. Whatâ€™s final loss & accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_6ZaB_xO7-AC"
      },
      "outputs": [],
      "source": [
        "# TODO: implement this\n",
        "def gradient_descent(X, y, lr=0.0001, steps=60000, verbose=True):\n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to find the optimal weights `w`.\n",
        "    \"\"\"\n",
        "    # Get the number of samples (m) and features (n)\n",
        "    m, n = X.shape\n",
        "    \n",
        "    # Initialize weights to zeros\n",
        "    w = np.zeros(n)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(steps):\n",
        "        # --- Forward Pass ---\n",
        "        # 1. Calculate the linear combination\n",
        "        z = X @ w\n",
        "        # 2. Get the predictions (probabilities)\n",
        "        y_hat = sigmoid(z)\n",
        "\n",
        "        # --- Gradient Calculation ---\n",
        "        # The gradient of the binary cross-entropy loss w.r.t. w\n",
        "        gradient = (1 / m) * X.T @ (y_hat - y)\n",
        "\n",
        "        # --- Weight Update ---\n",
        "        # Move weights in the opposite direction of the gradient\n",
        "        w = w - lr * gradient\n",
        "\n",
        "        # Log progress at specified intervals\n",
        "        if i % 20000 == 0 or i == steps - 1:\n",
        "            loss = compute_loss(X, y, w)\n",
        "            losses.append(loss)\n",
        "            if verbose:\n",
        "                print(f\"Step {i:>6} | Loss: {loss:.6f}\")\n",
        "\n",
        "    return w, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaDhW_J5mmPa",
        "outputId": "fabb8f42-444e-47df-c832-1e066df69418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step      0 | Loss: 0.693097\n",
            "Step  20000 | Loss: 0.596942\n",
            "Step  40000 | Loss: 0.596908\n",
            "Step  60000 | Loss: 0.596908\n",
            "Step  80000 | Loss: 0.596908\n",
            "Step 100000 | Loss: 0.596908\n",
            "Step 120000 | Loss: 0.596908\n",
            "Step 140000 | Loss: 0.596908\n",
            "Step 160000 | Loss: 0.596908\n",
            "Step 180000 | Loss: 0.596908\n",
            "Step 200000 | Loss: 0.596908\n",
            "Step 220000 | Loss: 0.596908\n",
            "Step 240000 | Loss: 0.596908\n",
            "Step 260000 | Loss: 0.596908\n",
            "Step 280000 | Loss: 0.596908\n",
            "Step 300000 | Loss: 0.596908\n",
            "Step 320000 | Loss: 0.596908\n",
            "Step 340000 | Loss: 0.596908\n",
            "Step 360000 | Loss: 0.596908\n",
            "Step 380000 | Loss: 0.596908\n",
            "Step 400000 | Loss: 0.596908\n",
            "Step 420000 | Loss: 0.596908\n",
            "Step 440000 | Loss: 0.596908\n",
            "Step 460000 | Loss: 0.596908\n",
            "Step 480000 | Loss: 0.596908\n",
            "Step 500000 | Loss: 0.596908\n",
            "Step 520000 | Loss: 0.596908\n",
            "Step 540000 | Loss: 0.596908\n",
            "Step 560000 | Loss: 0.596908\n",
            "Step 580000 | Loss: 0.596908\n",
            "Step 599999 | Loss: 0.596908\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "w, losses = gradient_descent(X_train, y_train, lr=0.001, steps=600000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgwj9-oRnUoJ",
        "outputId": "2a4fc982-224a-4e2d-d807-121aac7be275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.6625\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”¹ Test accuracy only\n",
        "y_test_pred = (sigmoid(X_test @ w) > 0.5).astype(int)\n",
        "accuracy = np.mean(y_test_pred == y_test)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93oWk2QKrx4x"
      },
      "source": [
        "### practice 6: analyze the Test accuracy. Is it good enough? Does training for longer epochs help? explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Practice 6: Analyze the Test Accuracy. Is it good enough?**\n",
        "\n",
        "**No**, an accuracy of ~66.25% is not considered good for this problem.\n",
        "\n",
        "To evaluate a classifier, we should compare it to a simple **baseline model**. For an imbalanced dataset, the most common baseline is the \"majority class classifier,\" which always predicts the most frequent class.\n",
        "\n",
        "1.  **Majority Class:** In our training data (and the overall dataset), the majority class is `0` (rejected), which makes up approximately **68.2%** of the samples.\n",
        "2.  **Baseline Accuracy:** A naive model that always predicts `0` would achieve an accuracy of `68.2%`.\n",
        "3.  **Comparison:** Our model's accuracy of `66.25%` is **worse than this trivial baseline**. This indicates that our model has failed to learn a meaningful pattern from the `gre` and `gpa` features alone.\n",
        "\n",
        "---\n",
        "**Does training for longer epochs help?**\n",
        "\n",
        "**No**, training for longer will not help.\n",
        "\n",
        "Looking at the training output from the `gradient_descent` function, we can see that the loss function has already **converged**.\n",
        "\n",
        "```text\n",
        "Step   20000 | Loss: 0.596942\n",
        "Step   40000 | Loss: 0.596908\n",
        "...\n",
        "Step  599999 | Loss: 0.596908"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss value stabilizes at `0.596908` and does not decrease further. This means the algorithm has already found the best possible parameters (`w`) that it can with the given data and learning rate. Continuing to train will not yield any improvement.\n",
        "\n",
        "---\n",
        "### **Explanation**\n",
        "\n",
        "The primary reason for the poor performance is likely the **oversimplification of the model**. We intentionally excluded the `rank` feature, which is a strong predictor for university admissions. With only `gre` and `gpa`, the data is not sufficiently linearly separable for the logistic regression model to find a good decision boundary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ai-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
