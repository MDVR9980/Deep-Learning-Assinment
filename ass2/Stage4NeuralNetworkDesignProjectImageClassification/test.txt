فایل اول: گزارش کار Stage 4 (پروژه طبقه‌بندی تصاویر)
عنوان فایل: Report_Stage4_ImageClassification.docx
گزارش پروژه طراحی شبکه عصبی: طبقه‌بندی تصاویر (CIFAR-10)
نام دانشجو: محمد داوود وهاب رجایی
شماره دانشجویی: 4041419041
درس: یادگیری عمیق
۱. مقدمه (Introduction)
۱.۱. بیان مسئله
هدف این پروژه طراحی، پیاده‌سازی و ارزیابی یک شبکه عصبی تمام متصل (Fully Connected Neural Network - FCN) برای طبقه‌بندی تصاویر چندکلاسه است. چالش اصلی در این پروژه، استفاده از یک معماری تمام متصل برای داده‌های تصویری است، جایی که معمولاً شبکه‌های کانولوشنی (CNN) عملکرد بهتری دارند. هدف ما بررسی میزان کارایی FCN و تأثیر تکنیک‌های مختلف مانند نرمال‌سازی دسته (Batch Normalization) و Dropout بر روی این داده‌ها است.
۱.۲. توصیف مجموعه داده
در این پروژه از مجموعه داده استاندارد CIFAR-10 استفاده شده است. این دیتاست شامل ۶۰,۰۰۰ تصویر رنگی با ابعاد ۳۲×۳۲ پیکسل در ۱۰ کلاس مختلف (هواپیما، خودرو، پرنده، گربه، گوزن، سگ، قورباغه، اسب، کشتی و کامیون) می‌باشد.
۲. روش‌شناسی (Methodology)
۲.۱. آماده‌سازی داده‌ها (Data Preparation)
پیش‌پردازش: تصاویر ابتدا به تانسورهای PyTorch تبدیل شدند. سپس نرمال‌سازی با میانگین ۰.۵ و انحراف معیار ۰.۵ انجام شد تا مقادیر پیکسل‌ها در بازه [-1, 1] قرار گیرند. این کار به همگرایی سریع‌تر مدل کمک می‌کند.
تقسیم‌بندی داده‌ها: داده‌های آموزشی (۵۰,۰۰۰ تصویر) به دو بخش آموزش (۸۵٪ معادل ۴۲,۵۰۰ تصویر) و اعتبارسنجی (۱۵٪ معادل ۷,۵۰۰ تصویر) تقسیم شدند. ۱۰,۰۰۰ تصویر نیز به عنوان داده تست جداگانه نگه‌داشته شد.
بارگذاری داده‌ها: از DataLoader با اندازه دسته (Batch Size) برابر با ۱۲۸ و قابلیت Shuffle برای داده‌های آموزشی استفاده شد.
۲.۲. معماری مدل (Model Architecture)
مدل طراحی شده یک شبکه تمام متصل (MLP) با ساختار زیر است:
لایه ورودی: تصویر ۳۲×۳۲×۳ به یک بردار مسطح (Flatten) با ابعاد ۳۰۷۲ تبدیل می‌شود.
لایه مخفی اول: ۵۱۲ نورون. شامل لایه Linear، سپس Batch Normalization، تابع فعال‌ساز LeakyReLU (با شیب منفی ۰.۱) و Dropout (با نرخ ۰.۵).
لایه مخفی دوم: ۲۵۶ نورون. مشابه لایه قبل شامل BN، LeakyReLU و Dropout.
لایه خروجی: ۱۰ نورون (متناظر با ۱۰ کلاس) که به تابع هزینه CrossEntropy متصل می‌شود.
تعداد کل پارامترهای قابل آموزش: ۱,۷۰۸,۸۱۰ پارامتر.
۲.۳. پیکربندی آموزش (Training Configuration)
تابع هزینه: CrossEntropyLoss.
بهینه‌ساز: Adam با نرخ یادگیری اولیه ۰.۰۰۱.
زمان‌بندی نرخ یادگیری (Scheduler): از ReduceLROnPlateau استفاده شد تا در صورت عدم بهبود دقت اعتبارسنجی، نرخ یادگیری نصف شود.
تعداد Epoch: مدل برای ۲۵ دور آموزش دید و بهترین مدل بر اساس دقت اعتبارسنجی ذخیره شد.
۳. نتایج تجربی (Experimental Results)
۳.۱. عملکرد آموزش و اعتبارسنجی
در طول فرآیند آموزش، نمودارهای Loss و Accuracy روند نزولی و صعودی مطلوبی داشتند. استفاده از Dropout و Batch Normalization باعث شد تا فرآیند یادگیری پایدارتر شود.
بهترین دقت اعتبارسنجی (Validation Accuracy): حدود ۵۵.۲۷٪ (در اپک ۲۵).
کاهش نرخ یادگیری توسط Scheduler در اپک‌های انتهایی به همگرایی دقیق‌تر کمک کرد.
۳.۲. عملکرد روی داده‌های تست (Test Performance)
پس از بارگذاری بهترین مدل ذخیره شده، ارزیابی نهایی روی داده‌های دیده نشده (Test Set) انجام شد:
دقت نهایی تست (Test Accuracy): ۵۵.۵۵٪
تحلیل ماتریس درهم‌ریختگی (Confusion Matrix): مدل در تشخیص کلاس‌هایی مانند "خودرو" و "کشتی" عملکرد خوبی داشت (F1-score حدود ۰.۶۷)، اما در تمایز حیوانات شبیه به هم (مانند "گربه" و "سگ") دچار اشتباه شد.
۴. بحث و تحلیل (Analysis and Discussion)
۴.۱. محدودیت‌های معماری FCN
دقت ۵۵٪ برای دیتاست CIFAR-10 با استفاده از یک شبکه تمام متصل، نتیجه‌ای قابل قبول و مورد انتظار است. دلیل اصلی عدم دستیابی به دقت‌های بالاتر (مانند ۹۰٪ که در CNNها دیده می‌شود)، از بین رفتن اطلاعات مکانی (Spatial Information) تصاویر هنگام مسطح کردن (Flattening) آن‌هاست. شبکه FCN نمی‌تواند روابط همسایگی پیکسل‌ها را به خوبی درک کند.
۴.۲. تأثیر تکنیک‌های تنظیم (Regularization)
استفاده از Dropout با نرخ ۰.۵ نقش کلیدی در جلوگیری از بیش‌برازش (Overfitting) شدید داشت. بدون این لایه، مدل احتمالاً داده‌های آموزشی را حفظ می‌کرد و دقت تست به شدت کاهش می‌یافت. همچنین Batch Normalization به سرعت همگرایی کمک شایانی کرد.
۵. نتیجه‌گیری و کارهای آتی
در این پروژه، یک خط لوله کامل یادگیری عمیق پیاده‌سازی شد. نتایج نشان داد که اگرچه شبکه‌های تمام متصل برای داده‌های تصویری پیچیده ایده‌آل نیستند، اما با تنظیم دقیق هایپرپارامترها و استفاده از تکنیک‌های مدرن مانند LeakyReLU و BatchNorm می‌توان به نتایج پایه خوبی دست یافت.
برای بهبود عملکرد در کارهای آتی، پیشنهاد می‌شود از معماری‌های کانولوشنی (CNN) استفاده شود و تکنیک‌های Data Augmentation (مانند چرخش و برش تصادفی تصاویر) به پیش‌پردازش اضافه گردد.